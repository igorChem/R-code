{
    "contents" : "# Caret packge \nvignette(package = \"caret\")\n\nlibrary(caret)\n\n#avNNet.default {caret}\tR Documentation\n# Neural Networks Using Model Averaging\n\ndata(BloodBrain)\n## Not run: \nmodelFit <- avNNet(bbbDescr, logBBB, size = 5, linout = TRUE, trace = FALSE)\nmodelFit\n\nann1 <-predict(modelFit, bbbDescr)\nplot(logBBB~ann1)\n\n\n#calibration\n#Probability Calibration Plot\n\n#Description\n#For classification models, this function creates a ’calibration plot’ that describes how consistent\n#model probabilities are with observed event rates.\n\ndata(mdrr)\nmdrrDescr <- mdrrDescr[, -nearZeroVar(mdrrDescr)]\nmdrrDescr <- mdrrDescr[, -findCorrelation(cor(mdrrDescr), .5)]\ninTrain <- createDataPartition(mdrrClass)\ntrainX <- mdrrDescr[inTrain[[1]], ]\ntrainY <- mdrrClass[inTrain[[1]]]\n\ntestX <- mdrrDescr[-inTrain[[1]], ]\ntestY <- mdrrClass[-inTrain[[1]]]\nlibrary(MASS)\nldaFit <- lda(trainX, trainY)\nqdaFit <- qda(trainX, trainY)\ntestProbs <- data.frame(obs = testY,\n                        lda = predict(ldaFit, testX)$posterior[,1],\n                        qda = predict(qdaFit, testX)$posterior[,1])\ncalibration(obs ~ lda + qda, data = testProbs)\ncalPlotData <- calibration(obs ~ lda + qda, data = testProbs)\ncalPlotData\nxyplot(calPlotData, auto.key = list(columns = 2))\n## End(Not run)\n\n#diff.resamples\n#Inferential Assessments About Model Performance\n#Description\n#Methods for making inferences about differences between models\n\n## Not run:\nload(url(\"http://topepo.github.io/caret/exampleModels.RData\"))\nresamps <- resamples(list(CART = rpartFit,\n                          CondInfTree = ctreeFit,\n                          MARS = earthFit))\ndifs <- diff(resamps)\ndifs\nsummary(difs)\ncompare_models(rpartFit, ctreeFit)\n## End(Not run)\n\n\n#dummyVars\n#Create A Full Set of Dummy Variables\n#Description\n#dummyVars creates a full set of dummy variables (i.e. less than full rank parameterization)\n\nwhen <- data.frame(time = c(\"afternoon\", \"night\", \"afternoon\",\n                            \"morning\", \"morning\", \"morning\",\n                            \"morning\", \"afternoon\", \"afternoon\"),\n                   day = c(\"Mon\", \"Mon\", \"Mon\",\n                           \"Wed\", \"Wed\", \"Fri\",\n                           \"Sat\", \"Sat\", \"Fri\"))\nlevels(when$time) <- list(morning=\"morning\",\n                          afternoon=\"afternoon\",\n                          night=\"night\")\nlevels(when$day) <- list(Mon=\"Mon\", Tue=\"Tue\", Wed=\"Wed\", Thu=\"Thu\",\n                         Fri=\"Fri\", Sat=\"Sat\", Sun=\"Sun\")\n## Default behavior:\nmodel.matrix(~day, when)\nmainEffects <- dummyVars(~ day + time, data = when)\nmainEffects\npredict(mainEffects, when[1:3,])\nwhen2 <- when\nwhen2[1, 1] <- NA\npredict(mainEffects, when2[1:3,])\npredict(mainEffects, when2[1:3,], na.action = na.omit)\ninteractionModel <- dummyVars(~ day + time + day:time,\n                              data = when,\n                              sep = \".\")\npredict(interactionModel, when[1:3,])\nnoNames <- dummyVars(~ day + time + day:time,\n                     data = when,\n                     levelsOnly = TRUE)\npredict(noNames, when)\ninteractionModel\n\n#featurePlot\n#Wrapper for Lattice Plotting of Predictor Variables\n\n\nx <- matrix(rnorm(50*5),ncol=5)\ny <- factor(rep(c(\"A\", \"B\"), 25))\ntrellis.par.set(theme = col.whitebg(), warn = FALSE)\nfeaturePlot(x, y, \"ellipse\")\nfeaturePlot(x, y, \"strip\", jitter = TRUE)\nfeaturePlot(x, y, \"box\")\nfeaturePlot(x, y, \"pairs\")\n\n# gafs.default\n# Genetic algorithm feature selection\n# Description\n# Supervised feature selection using genetic algorithms\n\n\nset.seed(1)\ntrain_data <- twoClassSim(100, noiseVars = 10)\ntest_data <- twoClassSim(10, noiseVars = 10)\n## A short example\nctrl <- gafsControl(functions = rfGA,\n                    method = \"cv\",\n                    number = 3)\nrf_search <- gafs(x = train_data[, -ncol(train_data)],\n                  y = train_data$Class,\n                  iters = 3,\n                  gafsControl = ctrl)\nrf_search\n## End(Not run)\n\nset.seed(1)\ntrain_data <- twoClassSim(100, noiseVars = 10)\ntest_data <- twoClassSim(10, noiseVars = 10)\n## A short example\nctrl <- gafsControl(functions = rfGA,\n                    method = \"cv\",\n                    number = 3)\nrf_search <- gafs(x = train_data[, -ncol(train_data)],\n                  y = train_data$Class,\n                  iters = 3,\n                  gafsControl = ctrl)\nrf_search\n## End(Not run)\nset.seed(1)\ntrain_data <- twoClassSim(100, noiseVars = 10)\ntest_data <- twoClassSim(10, noiseVars = 10)\n## A short example\nctrl <- gafsControl(functions = rfGA,\n                    method = \"cv\",\n                    number = 3)\nrf_search <- gafs(x = train_data[, -ncol(train_data)],\n                  y = train_data$Class,\n                  iters = 3,\n                  gafsControl = ctrl)\nrf_search\n## End(Not run)\n\nmodelLookup()\n\n#train\n# fit Predictive Models over Different Tuning Parameters\n# Description\n# This function sets up a grid of tuning parameters for a number of classification and regression\n# routines, fits each model and calculates a resampling based performance measure.\n\n\n\n#######################################\n## Classification Example\ndata(iris)\nTrainData <- iris[,1:4]\nTrainClasses <- iris[,5]\n\nknnFit1 <- train(TrainData, TrainClasses,\n                 method = \"knn\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneLength = 10,\n                 trControl = trainControl(method = \"cv\"))\n\n\nknnFit1\n\nknnFit2 <- train(TrainData, TrainClasses,\n                 method = \"knn\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneLength = 10,\n                 trControl = trainControl(method = \"boot\"))\nknnFit2\n\n\nlibrary(MASS)\nnnetFit <- train(TrainData, TrainClasses,\n                 method = \"nnet\",\n                 preProcess = \"range\",\n                 tuneLength = 2,\n                 trace = FALSE,\n                 maxit = 100)\nnnetFit\n\n\n## Regression Example\nlibrary(mlbench)\ndata(BostonHousing)\nlmFit <- train(medv ~ . + rm:lstat,\n               data = BostonHousing,\n               method = \"lm\")\n\nlmFit\n\nlibrary(rpart)\nrpartFit <- train(medv ~ .,\n                  data = BostonHousing,\n                  method = \"rpart\",\n                  tuneLength = 9)\n\n\n# create data partition \nlibrary(mlbench)\n data(Sonar)\nset.seed(107)\n inTrain <- createDataPartition(y = Sonar$Class,\n                                 + ## the outcome data are needed\n                                   +  p = .75,\n                                 +  ## The percentage of data in the\n                                   +   ## training set\n                                   +  list = FALSE)\n                               ## The format of the results\n  \n## The output is a set of integers for the rows of Sonar\n## that belong in the training set.\nstr(inTrain)\ntraining <- Sonar[ inTrain,]\ntesting  <- Sonar[-inTrain,]\n\n#pcaNNet.default\n#Neural Networks with a Principal Component Step\n\n#Description\n#Run PCA on a dataset, then use it in a neural network model\n\ndata(BloodBrain)\nmodelFit <- pcaNNet(bbbDescr[, 1:10], logBBB, size = 5, linout = TRUE, trace = FALSE)\nmodelFit\n\nsummary(modelFit)\nfr <-predict(modelFit, bbbDescr[, 1:10])\nplot(logBBB~fr)\n\n\n## detalhes dos modelos para utilizar o train \n\n\n#glm {stats}\n#Fitting Generalized Linear Models\n\n## Dobson (1990) Page 93: Randomized Controlled Trial :\ncounts <- c(18,17,15,20,10,20,25,13,12)\noutcome <- gl(3,1,9) # generate factor levels\ntreatment <- gl(3,3)\nprint(d.AD <- data.frame(treatment, outcome, counts)) # mostra a data.frame \nglm.D93 <- glm(counts ~ outcome+treatment, family = poisson())\nanova(glm.D93)\nsummary(glm.D93)\n\n#lssvm {kernlab}\tR Documentation\n#Least Squares Support Vector Machine\n\nlibrary(kernlab)\ndata(iris)\nlir <- lssvm(Species~.,data=iris)\nll <-lssvm(Sepal.Length~.,data=iris)\nlir\nlirr <- lssvm(Species~.,data= iris, reduced = FALSE)\nlirr\n## Using the kernelMatrix interface\niris <- unique(iris)\nrbf <- rbfdot(0.5)\nk <- kernelMatrix(rbf, as.matrix(iris[,-5]))\nklir <- lssvm(k, iris[, 5])\nklir\npre <- predict(klir, k)\nplot(iris)\nnames(iris)\nattach(iris)\nlre <-lssvm(Species~Sepal.Length*Sepal.Length,data=iris)\nlre\nplot(pre~Species)\ncoefficients(klir)\niris\n\n\n\n#ksvm\n\n## simple example using the spam data set\ndata(spam)\n## create test and training set\nindex <- sample(1:dim(spam)[1])\nspamtrain <- spam[index[1:floor(dim(spam)[1]/2)], ]\nspamtest <- spam[index[((ceiling(dim(spam)[1]/2)) + 1):dim(spam)[1]], ]\n## train a support vector machine\nfilter <- ksvm(type~.,data=spamtrain,kernel=\"rbfdot\",\n               kpar=list(sigma=0.05),C=5,cross=3)\nfilter\n## predict mail type on the test set\nmailtype <- predict(filter,spamtest[,-58])\n## Check results\ntable(mailtype,spamtest[,58])\n\n\n#krls {KRLS}\tR Documentation\n#Kernel-based Regularized Least Squares (KRLS)\n\n# non-linear example\n# set up data\nN <- 200\nx1 <- rnorm(N)\nx2 <- rbinom(N,size=1,prob=.2)\ny <- x1^3 + .5*x2 + rnorm(N,0,.15)\nX <- cbind(x1,x2)\n\n# fit model\nkrlsout <- krls(X=X,y=y)\n# summarize marginal effects and contribution of each variable\nsummary(krlsout)\n# plot marginal effects and conditional expectation plots\nplot(krlsout)\n\n",
    "created" : 1455738002894.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1437132876",
    "id" : "F82F7FD8",
    "lastKnownWriteTime" : 1456359096,
    "path" : "~/r/Rlearning/Caret.R",
    "project_path" : "Caret.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_source"
}